#!/usr/bin/env python3
"""
Extract clean output from NiFi Databricks Agent result
Converts the raw agent output into readable format
"""

import os
import json

# ================================================================
# CONFIGURATION - UPDATE THESE PARAMETERS
# ================================================================

# Databricks Configuration
# DO NOT hardcode secrets. Read the Databricks token and host from environment variables.
DATABRICKS_TOKEN = os.environ.get("DATABRICKS_TOKEN")
DATABRICKS_HOST = os.environ.get("DATABRICKS_HOST", "adb-5721858900606423.3.azuredatabricks.net")

# File Paths
XML_FILE = "/Workspace/Users/eliao@bpcs.com/create_agent_using_langGraph/nifi_pipeline_eric_1.xml"
OUTPUT_FILE = "nifi_migration_report.md"

# ================================================================
# END CONFIGURATION
# ================================================================

# Set credentials
os.environ["DATABRICKS_TOKEN"] = DATABRICKS_TOKEN
os.environ["DATABRICKS_HOST"] = DATABRICKS_HOST

def extract_clean_output(result):
    """Extract clean, readable output from agent result"""
    
    print("="*70)
    print("ðŸ”„ EXTRACTING CLEAN OUTPUT FROM AGENT")
    print("="*70)
    
    # Initialize sections
    parsed_data = {}
    databricks_code = {}
    job_config = {}
    final_message = ""
    
    if hasattr(result, 'output') and result.output:
        for output in result.output:
            
            # Function call outputs (tool results)
            if hasattr(output, 'type'):
                if output.type == 'function_call_output':
                    # Parse JSON outputs from tools
                    try:
                        json_output = json.loads(output.output)
                        
                        # Check if it's parsed NiFi data
                        if 'processors' in json_output and 'connections' in json_output:
                            parsed_data = json_output
                            
                        # Check if it's job config
                        elif 'name' in json_output and 'tasks' in json_output:
                            job_config = json_output
                            
                    except (json.JSONDecodeError, AttributeError):
                        # Check if it's code generation output
                        if hasattr(output, 'output') and ('â†’' in output.output or 'format(' in output.output):
                            # Extract processor name from comment
                            lines = output.output.split('\n')
                            processor_name = lines[0].split(' â†’')[0].replace('# ', '') if lines else 'Unknown'
                            databricks_code[processor_name] = output.output
                
                # Final message from agent
                elif output.type == 'message':
                    if hasattr(output, 'content') and output.content:
                        for content_item in output.content:
                            if content_item.get('type') == 'output_text':
                                final_message = content_item.get('text', '')
    
    return parsed_data, databricks_code, job_config, final_message

def format_migration_report(parsed_data, databricks_code, job_config, final_message):
    """Format the extracted data into a clean migration report"""
    
    report = []
    report.append("# NiFi to Databricks Migration Report")
    report.append("*Generated by NiFi Databricks Agent*")
    report.append("")
    report.append("---")
    report.append("")
    
    # 1. NiFi Template Analysis
    if parsed_data:
        report.append("## ðŸ“‹ NiFi Template Analysis")
        report.append("")
        report.append(f"**Total Processors:** {parsed_data.get('processor_count', 0)}")
        report.append(f"**Total Connections:** {parsed_data.get('connection_count', 0)}")
        report.append("")
        
        if 'processors' in parsed_data:
            report.append("### Processors Found:")
            for proc in parsed_data['processors']:
                report.append(f"- **{proc['name']}** (`{proc['type']}`)")
                if proc.get('properties'):
                    for key, value in list(proc['properties'].items())[:3]:  # Show first 3 properties
                        report.append(f"  - {key}: `{value}`")
            report.append("")
    
    # 2. Databricks Code Generation
    if databricks_code:
        report.append("## ðŸ”„ Databricks Migration Code")
        report.append("")
        for processor, code in databricks_code.items():
            report.append(f"### {processor}")
            report.append("```python")
            report.append(code)
            report.append("```")
            report.append("")
    
    # 3. Job Configuration
    if job_config:
        report.append("## âš™ï¸ Databricks Job Configuration")
        report.append("")
        report.append("```json")
        report.append(json.dumps(job_config, indent=2))
        report.append("```")
        report.append("")
    
    # 4. Migration Summary
    if final_message:
        report.append("## ðŸ“ Migration Summary")
        report.append("")
        report.append(final_message)
        report.append("")
    
    # 5. Complete Pipeline Example
    report.append("## ðŸš€ Complete Databricks Pipeline")
    report.append("")
    report.append("```python")
    report.append("# Complete NiFi to Databricks Migration")
    report.append("from pyspark.sql.functions import *")
    report.append("from pyspark.sql.types import *")
    report.append("")
    report.append("# Auto Loader (replaces NiFi GetFile)")
    report.append("df = spark.readStream \\")
    report.append("  .format('cloudFiles') \\")
    report.append("  .option('cloudFiles.format', 'csv') \\")
    report.append("  .option('cloudFiles.schemaLocation', '/tmp/schema') \\")
    report.append("  .option('cloudFiles.maxFilesPerTrigger', 10) \\")
    report.append("  .load('/opt/nifi/nifi-current/data')")
    report.append("")
    report.append("# Data processing (add your transformations here)")
    report.append("processed_df = df.filter(col('_c0').isNotNull())  # Example filter")
    report.append("")
    report.append("# Delta Lake Write (replaces NiFi PutHDFS)")
    report.append("processed_df.writeStream \\")
    report.append("  .format('delta') \\")
    report.append("  .outputMode('append') \\")
    report.append("  .option('checkpointLocation', '/tmp/checkpoint') \\")
    report.append("  .start('/user/nifi/sensor_data')")
    report.append("```")
    report.append("")
    
    return "\n".join(report)

def main():
    """Main function to demonstrate clean output extraction"""
    
    print("ðŸ”„ Loading NiFi XML template...")
    with open(XML_FILE, "r", encoding="utf-8") as f:
        xml_content = f.read()
    
    print("ðŸ¤– Running NiFi Databricks Agent...")
    from nifi_databricks_agent import AGENT
    
    user_prompt = f"""
    I have a NiFi XML pipeline. Please:
    1. Parse the processors and connections
    2. For each processor, generate equivalent Databricks PySpark code
    3. Create a complete Databricks job configuration
    4. Provide migration recommendations

    NiFi XML:
    ```xml
    {xml_content}
    ```
    """

    result = AGENT.predict({
        "input": [{"role": "user", "content": user_prompt}]
    })
    
    # Extract clean output
    parsed_data, databricks_code, job_config, final_message = extract_clean_output(result)
    
    # Generate report
    report = format_migration_report(parsed_data, databricks_code, job_config, final_message)
    
    # Save to file
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("\n" + "="*70)
    print("âœ… CLEAN MIGRATION REPORT GENERATED")
    print("="*70)
    print(f"ðŸ“„ Saved to: {OUTPUT_FILE}")
    print("\nðŸ“‹ Report Preview:")
    print("-" * 70)
    print(report[:2000] + "..." if len(report) > 2000 else report)

if __name__ == "__main__":
    main()